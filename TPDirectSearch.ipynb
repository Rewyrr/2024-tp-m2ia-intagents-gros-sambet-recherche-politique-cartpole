{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche directe de politique\n",
    "\n",
    "Dans ce TP, l'objectif est d'implémenter un algorithme de recherche directe de politique pour trouver une politique optimale pour l'environnement [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "> Vous devez  <span style=\"color:red\"> commenter votre code ! </span> \n",
    "> \n",
    "> Vous devez aussi répondre à des questions (dans les cellules <span style=\"color:blue\">Votre réponse: </span> )\n",
    "\n",
    "\n",
    "> Le code doit être fonctionnel avec l'environnement virtuel du TP. Si d'autres packages que ceux présents dans l'environnement virtuel créé au départ sont nécessaires, vous devez ajouter à votre dépôt un fichier `environnement.yaml` qui est un export de votre environnement virtuel. Ce fichier est obtenu avec la commande suivante:  ```conda env export > environnement.yaml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "def init_seed(seedval):\n",
    "    torch.manual_seed(seedval)\n",
    "    np.random.seed(seedval)\n",
    "    random.seed(seedval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Environnement CartPole\n",
    "\n",
    "Lire la documentation et afficher les caractéristiques (états, action) de l'environnement *CartPole*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Caractéristiques de l'environnement CartPole ===\n",
      "Espace d'états (observations): Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      " - Borne inférieure des états: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      " - Borne supérieure des états: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "\n",
      "Espace d'actions: Discrete(2)\n",
      " - Nombre d'actions possibles: 2\n",
      " - Actions possibles: [0, 1] (0: Gauche, 1: Droite)\n",
      "\n",
      "Critères de fin d'épisode:\n",
      " - Le poteau dépasse un angle de ±12° (environ 0.418 radians).\n",
      " - Le chariot dépasse la limite de ±2.4 mètres.\n",
      " - Le nombre maximum de pas de temps atteint (généralement 500).\n",
      "\n",
      "Récompense:\n",
      " - Récompense de +1 à chaque pas tant que le poteau reste en équilibre.\n"
     ]
    }
   ],
   "source": [
    "# Initialiser l'environnement CartPole\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Obtenir les caractéristiques de l'environnement\n",
    "# Espace d'état (observation)\n",
    "state_space = env.observation_space\n",
    "# Espace d'actions\n",
    "action_space = env.action_space\n",
    "\n",
    "# Afficher les informations sur l'environnement\n",
    "print(\"=== Caractéristiques de l'environnement CartPole ===\")\n",
    "print(f\"Espace d'états (observations): {state_space}\")\n",
    "print(f\" - Borne inférieure des états: {state_space.low}\")\n",
    "print(f\" - Borne supérieure des états: {state_space.high}\")\n",
    "\n",
    "print(f\"\\nEspace d'actions: {action_space}\")\n",
    "print(f\" - Nombre d'actions possibles: {action_space.n}\")\n",
    "print(f\" - Actions possibles: {list(range(action_space.n))} (0: Gauche, 1: Droite)\")\n",
    "\n",
    "print(\"\\nCritères de fin d'épisode:\")\n",
    "print(\" - Le poteau dépasse un angle de ±12° (environ 0.418 radians).\")\n",
    "print(\" - Le chariot dépasse la limite de ±2.4 mètres.\")\n",
    "print(\" - Le nombre maximum de pas de temps atteint (généralement 500).\")\n",
    "\n",
    "print(\"\\nRécompense:\")\n",
    "print(\" - Récompense de +1 à chaque pas tant que le poteau reste en équilibre.\")\n",
    "\n",
    "# Fermer l'environnement\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Définition de la Politique\n",
    "\n",
    "\n",
    "> <span style=\"color:green\">- Quelle est la différence entre une politique stochastique et déterministe ?  </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>\n",
    "\n",
    "Une politique déterministe associe chaque état à une action unique. Cela signifie que pour chaque état donné, l'agent choisit toujours la même action.\n",
    "\n",
    "Une politique stochastique associe chaque état à une distribution de probabilité sur les actions possibles. Cela signifie que l'agent peut choisir différentes actions à partir d'un même état, avec des probabilités spécifiques.\n",
    "\n",
    "Dans le cas déterministe l'agent va toujours faire le meme choix alors que dans le cas stochastique l'agent va parfois changer son action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\">- Dans le cas d'une politique déterministe dans le CartPole, qu'est-ce qui sera en entrée de la politique ? En sortie ?  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>\n",
    "\n",
    "l'entrée est le vecteur d'état qui décrit la situation actuelle du système, et la sortie est l'action unique à entreprendre pour maintenir l'équilibre du poteau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\">- Dans le cas d'une politique stochastique dans le CartPole, qu'est-ce qui sera en entrée de la politique ? En sortie ?  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>\n",
    "\n",
    "l'entrée est le vecteur d'état qui décrit la situation actuelle du système, tandis que la sortie est une distribution de probabilités sur les actions possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter la classe *Politique* ci-dessous qui sera utilisée dans le CartPole. Cette politique sera déterministe ou stochastique (choix via l'argument *det*) et sera composée d'une seule couche de poids (il est conseillé d'utiliser une *ndarray* pour les poids car pas besoin de calcul de gradient). Vous devez utiliser une distribution catégorielle de *Torch* pour la politique stochastique. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Politique():\n",
    "\n",
    "    def __init__(self, dim_entree:int, dim_sortie:int, det = True):\n",
    "        \n",
    "        #TO DO \n",
    "\n",
    "        self.dim_entree = dim_entree\n",
    "        self.dim_sortie = dim_sortie\n",
    "        self.det = det\n",
    "\n",
    "        self.poids = np.random.randn(dim_entree, dim_sortie) * 0.01\n",
    " \n",
    "    def output(self, etat : np.ndarray) -> int:\n",
    "\n",
    "        #TO DO\n",
    "        logits = etat @ self.poids  # Produit matriciel\n",
    "\n",
    "        if self.det:\n",
    "            return int(np.argmax(logits))\n",
    "        else:\n",
    "            probas = F.softmax(torch.tensor(logits), dim=0)  # Calcul des probabilités\n",
    "            action = torch.multinomial(probas, num_samples=1)  # Échantillonnage d'une action\n",
    "            return int(action.item())\n",
    " \n",
    " \n",
    "    def set_poids(self, poids: np.ndarray):\n",
    "        self.poids = poids\n",
    "    \n",
    "    def get_poids(self) -> np.ndarray:\n",
    "        return self.poids\n",
    "\n",
    "    def save(self,file):\n",
    "        np.save(file, self.poids)\n",
    "\n",
    "    def load(self,file):\n",
    "        self.poids = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter la méthode *eval_politique* qui évalue une politique sur plusieurs épisodes, et renvoie la **performance de chaque épisode**, qui est la somme des récompenses obtenues sur chaque épisode.\n",
    "\n",
    "NB: L'attribut 'init_large' permet d'élargir les bornes définies pour le choix de l'état initial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_politique(politique, env, nb_episodes=100, max_t=1000, seed=random.randint(0,5000), init_large=False) -> list:\n",
    "\n",
    "\n",
    "    somme_rec = []\n",
    "    for epi in range(1, nb_episodes+1):\n",
    "        rec_epi = []\n",
    "        if init_large: \n",
    "            state, _ = env.reset(seed=seed,options={'low':-0.2,'high':0.2})\n",
    "        else:\n",
    "            state, _ = env.reset(seed=seed)\n",
    "        #TO COMPLETE\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action = politique.output(state)  # Obtenir l'action de la politique\n",
    "            state, reward, done, info, x = env.step(action)  # Appliquer l'action et obtenir les nouveaux états et récompenses\n",
    "            \n",
    "            rec_epi.append(reward)  # Ajouter la récompense à la récompense de l'épisode\n",
    "            \n",
    "            if done:  # Vérifier si l'épisode est terminé\n",
    "                break\n",
    "        \n",
    "        somme_rec.append(len(rec_epi))  # Ajouter la récompense de l'épisode à la liste des performances\n",
    "    \n",
    "    return somme_rec  # Retourner la liste des performances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On évalue ci-dessous une politique déterministe et une politique stochastique (non-optimisées) sur 5 épisodes, **à partir du même état initial**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf  = [48, 48, 48, 48, 48]\n",
      "Perf  = [29, 11, 11, 29, 22]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1',render_mode=\"human\")\n",
    "init_seed(5)\n",
    "\n",
    "politique = Politique(dim_entree=4, dim_sortie=2,det=True) \n",
    "perf = eval_politique(politique,env,seed=5,nb_episodes=5)\n",
    "print(f\"Perf  = {perf}\")\n",
    "\n",
    "politique = Politique(dim_entree=4, dim_sortie=2,det=False) \n",
    "perf = eval_politique(politique,env,seed=5,nb_episodes=5)\n",
    "print(f\"Perf  = {perf}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\">- Analyser les résultats obtenus sur les 2 évaluations: quelle est la différence ? pourquoi ? que peut-on en déduire sur la stochasticité de l'environnement Cart-Pole ?  </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>\n",
    "\n",
    "La différence est que le déterministe est prévisible et obtient toujours le même résultat alors que le stochastique change.\n",
    "Car le stochastique utilise des probabilités pour faire son choix alors que le déterministe prend le meilleure.\n",
    "\n",
    "On peut déduire que la stochasticité de l'environnement n'est pas bonne car en déterministe on obtient de meilleure résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Algorithme de recherche directe de politique\n",
    "\n",
    "L'objectif est d'implément un algorithme de type *Adaptive Noise Scaling*, qui va à chaque itération, générer une nouvelle politique candidate en perturbant les paramètres de la meilleure politique trouvée. \n",
    "\n",
    "La perturbation des paramètres sera réalisée en ajoutant un *bruit* sur chaque paramètre, et en faisant **varier la variance** de ce bruit: variance réduite si la politique candidate trouvée est meilleure que l'ancienne, augmentée sinon.\n",
    "\n",
    "L'algorithme est le suivant:\n",
    "- Choix d'une politique initiale arbitraire Pi et évaluation de cette politique\n",
    "- Tant que l'environnement n'est pas résolu (itération)\n",
    "    - Génération d'une nouvelle politique candidate à partir de Pi\n",
    "    - Evaluation de la candidate\n",
    "    - Si la candidate a une meilleure perf que Pi: réduction de la variance du bruit et Pi = Candidate\n",
    "    - Si la candidate a une moins bonne perf que Pi: augmentation de la variance du bruit \n",
    "\n",
    "\n",
    "> <span style=\"color:green\">Vous devez  </span>\n",
    "- compléter la méthode *rollout* qui se charge d'évaluer une politique sur un épisode\n",
    "- compléter la méthode *recherche_directe* pour faire une exploration dans l'espace des paramètres de la politique, sur *nb_episode* de durée max *max_t* pas. Cette méthode renverra les performances obtenues à chaque itération et les poids de la meilleure politique trouvée\n",
    "- faire en sorte de stopper la recherche lorsqu'une politique optimale est trouvée\n",
    "- afficher les performances régulièrement pendant la recherche\n",
    "- vous utiliserez ici des états initiaux différents à chaque épisode (le paramètre *seed* lors du *reset* de l'environnement doit être différent à chaque épisode, par ex., avec *random.randint(0,5000)* pour obtenir des expériences répétables ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(politique, env,max_t=1000)-> int : \n",
    "    \"\"\"\n",
    "        execute un episode sur l'environnement env avec la politique et renvoie la somme des recompenses obtenues sur l'épisode\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "    state, _ = env.reset(seed=random.randint(0, 5000)) \n",
    "    total_reward = 0\n",
    "    for t in range(max_t):\n",
    "        action = politique.output(state) \n",
    "        state, reward, done, trunc, info = env.step(action) \n",
    "        total_reward += reward\n",
    "        if done: \n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def recherche_directe(politique, env,  nb_episodes=5000, max_t=1000) -> tuple[list,np.ndarray] :\n",
    "    #TO COMPLETE\n",
    "    bruit_std = 1e-2 \n",
    "    performances = []\n",
    "    meilleur_perf = -np.inf  \n",
    "    meilleur_politique = politique.get_poids() \n",
    "    best_poli = politique\n",
    "    count = 0\n",
    "\n",
    "    for i_episode in range(1, nb_episodes + 1):\n",
    "        \n",
    "        politique_bruitee = best_poli\n",
    "        #print(best_poli.get_poids())\n",
    "        bruit = np.random.normal(0, bruit_std, size=politique_bruitee.get_poids().shape)\n",
    "        politique_bruitee.set_poids(best_poli.get_poids() + bruit)\n",
    "        \n",
    "        # Évaluer la performance de la politique bruitée\n",
    "        perf = rollout(politique_bruitee, env, max_t)\n",
    "        performances.append(perf)\n",
    "\n",
    "        if perf >= meilleur_perf:  \n",
    "            best_poli = politique_bruitee\n",
    "            meilleur_perf = perf\n",
    "            meilleur_politique = politique_bruitee.get_poids()\n",
    "            bruit_std = 1e-2 # reset de la variance quand une meilleur perf est trouvé pour chercher autour\n",
    "            # Réduction de la variance du bruit\n",
    "            bruit_std = max(1e-3, bruit_std / 2)\n",
    "        else:\n",
    "            # Augmentation de la variance du bruit\n",
    "            bruit_std = max(2, bruit_std * 2) # avec max 2 en variance, pas assez élevé pour trouver la solution\n",
    "\n",
    "        if i_episode % 1 == 0:\n",
    "            print(f\"Épisode {i_episode}/{nb_episodes} : Perf = {perf}, Meilleure Perf = {meilleur_perf}, Bruit Std = {bruit_std}\")\n",
    "            \n",
    "        if meilleur_perf >= env.spec.reward_threshold:  #si il trouve 5 fois la meilleur perf on peut s'arrêter\n",
    "            count+=1\n",
    "            if count == 5:\n",
    "                print(f\"Politique optimale trouvée à l'épisode {i_episode} avec une performance de {meilleur_perf}\")\n",
    "                break\n",
    "\n",
    "\n",
    "    return performances, meilleur_politique\n",
    "            \n",
    "         \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter la cellule ci-dessous pour sauvegarder votre meilleure politique déterministe dans un fichier \"best_pi_det.npy\" et votre meilleure politique stochastique dans un fichier \"best_pi_stoc.npy\".\n",
    "\n",
    "<span style=\"color:red\"> Vous ajouterez ces 2 fichiers dans votre dépôt github ! </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 1/5000 : Perf = 795.0, Meilleure Perf = 795.0, Bruit Std = 0.005\n",
      "Épisode 2/5000 : Perf = 88.0, Meilleure Perf = 795.0, Bruit Std = 2\n",
      "Épisode 3/5000 : Perf = 1000.0, Meilleure Perf = 1000.0, Bruit Std = 0.005\n",
      "Épisode 4/5000 : Perf = 1000.0, Meilleure Perf = 1000.0, Bruit Std = 0.005\n",
      "Épisode 5/5000 : Perf = 1000.0, Meilleure Perf = 1000.0, Bruit Std = 0.005\n",
      "Politique optimale trouvée à l'épisode 5 avec une performance de 1000.0\n",
      "Épisode 1/5000 : Perf = 20.0, Meilleure Perf = 20.0, Bruit Std = 0.005\n",
      "Épisode 2/5000 : Perf = 30.0, Meilleure Perf = 30.0, Bruit Std = 0.005\n",
      "Épisode 3/5000 : Perf = 22.0, Meilleure Perf = 30.0, Bruit Std = 2\n",
      "Épisode 4/5000 : Perf = 47.0, Meilleure Perf = 47.0, Bruit Std = 0.005\n",
      "Épisode 5/5000 : Perf = 34.0, Meilleure Perf = 47.0, Bruit Std = 2\n",
      "Épisode 6/5000 : Perf = 69.0, Meilleure Perf = 69.0, Bruit Std = 0.005\n",
      "Épisode 7/5000 : Perf = 76.0, Meilleure Perf = 76.0, Bruit Std = 0.005\n",
      "Épisode 8/5000 : Perf = 73.0, Meilleure Perf = 76.0, Bruit Std = 2\n",
      "Épisode 9/5000 : Perf = 34.0, Meilleure Perf = 76.0, Bruit Std = 4\n",
      "Épisode 10/5000 : Perf = 60.0, Meilleure Perf = 76.0, Bruit Std = 8\n",
      "Épisode 11/5000 : Perf = 10.0, Meilleure Perf = 76.0, Bruit Std = 16\n",
      "Épisode 12/5000 : Perf = 9.0, Meilleure Perf = 76.0, Bruit Std = 32\n",
      "Épisode 13/5000 : Perf = 18.0, Meilleure Perf = 76.0, Bruit Std = 64\n",
      "Épisode 14/5000 : Perf = 49.0, Meilleure Perf = 76.0, Bruit Std = 128\n",
      "Épisode 15/5000 : Perf = 23.0, Meilleure Perf = 76.0, Bruit Std = 256\n",
      "Épisode 16/5000 : Perf = 28.0, Meilleure Perf = 76.0, Bruit Std = 512\n",
      "Épisode 17/5000 : Perf = 60.0, Meilleure Perf = 76.0, Bruit Std = 1024\n",
      "Épisode 18/5000 : Perf = 9.0, Meilleure Perf = 76.0, Bruit Std = 2048\n",
      "Épisode 19/5000 : Perf = 9.0, Meilleure Perf = 76.0, Bruit Std = 4096\n",
      "Épisode 20/5000 : Perf = 10.0, Meilleure Perf = 76.0, Bruit Std = 8192\n",
      "Épisode 21/5000 : Perf = 62.0, Meilleure Perf = 76.0, Bruit Std = 16384\n",
      "Épisode 22/5000 : Perf = 25.0, Meilleure Perf = 76.0, Bruit Std = 32768\n",
      "Épisode 23/5000 : Perf = 1000.0, Meilleure Perf = 1000.0, Bruit Std = 0.005\n",
      "Épisode 24/5000 : Perf = 83.0, Meilleure Perf = 1000.0, Bruit Std = 2\n",
      "Épisode 25/5000 : Perf = 74.0, Meilleure Perf = 1000.0, Bruit Std = 4\n",
      "Épisode 26/5000 : Perf = 92.0, Meilleure Perf = 1000.0, Bruit Std = 8\n",
      "Épisode 27/5000 : Perf = 1000.0, Meilleure Perf = 1000.0, Bruit Std = 0.005\n",
      "Politique optimale trouvée à l'épisode 27 avec une performance de 1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lolol\\anaconda3\\envs\\tpdeeprl2024\\lib\\site-packages\\numpy\\lib\\npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "init_seed(5)\n",
    "\n",
    "#TODO\n",
    "\n",
    "politiqueDet = Politique(dim_entree=4, dim_sortie=2,det=True) \n",
    "politiqueSto = Politique(dim_entree=4, dim_sortie=2,det=False) \n",
    "\n",
    "best_pi_det = recherche_directe(politiqueDet,env,5000,1000)  \n",
    "best_pi_stoc = recherche_directe(politiqueSto,env,5000,1000)\n",
    "\n",
    "\n",
    "# Sauvegarde des politiques\n",
    "np.save(\"best_pi_det.npy\", best_pi_det)  # Sauvegarder la politique déterministe\n",
    "np.save(\"best_pi_stoc.npy\", best_pi_stoc)  # Sauvegarder la politique stochastique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Evaluation des politiques trouvées\n",
    "\n",
    "Compléter la cellule ci-dessous pour évaluer les meilleures politiques déterministes et stochastiques que vous avez trouvées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation de la meilleure politique déterministe:\n",
      "Performance moyenne sur 100 épisodes: 888.47\n",
      "\n",
      "Évaluation de la meilleure politique stochastique:\n",
      "Performance moyenne sur 100 épisodes: 165.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "# Charger les meilleures politiques sauvegardées\n",
    "politique_det = Politique(dim_entree=4, dim_sortie=2, det=True)\n",
    "politique_stoc = Politique(dim_entree=4, dim_sortie=2, det=False)\n",
    "\n",
    "# Charger les poids des fichiers .npy pour chaque politique\n",
    "poids_det = np.load(\"best_pi_det.npy\", allow_pickle=True)\n",
    "poids_stoc = np.load(\"best_pi_stoc.npy\", allow_pickle=True)\n",
    "\n",
    "# Convertir les poids en tableau NumPy de forme correcte (sinon problème de dimensions)\n",
    "poids_det = np.array(poids_det[1])\n",
    "poids_stoc = np.array(poids_stoc[1])\n",
    "\n",
    "# Vérification des dimensions (on avait un problème de dimensions)\n",
    "if poids_det.shape != (4, 2):\n",
    "    raise ValueError(f\"Dimensions des poids déterministes incorrectes : {poids_det.shape}, attendu (4, 2)\")\n",
    "if poids_stoc.shape != (4, 2):\n",
    "    raise ValueError(f\"Dimensions des poids stochastiques incorrectes : {poids_stoc.shape}, attendu (4, 2)\")\n",
    "\n",
    "# Mettre les poids dans les politiques\n",
    "politique_det.set_poids(poids_det)\n",
    "politique_stoc.set_poids(poids_stoc)\n",
    "\n",
    "# Fonction d'évaluation d'une politique\n",
    "def evaluer_politique(politique, env, nb_episodes=100, max_t=1000):\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(nb_episodes):\n",
    "        state, _ = env.reset(seed=random.randint(0, 5000))\n",
    "        episode_reward = 0\n",
    "        for t in range(max_t):\n",
    "            action = politique.output(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        total_rewards.append(episode_reward)\n",
    "    moyenne_perf = np.mean(total_rewards)\n",
    "    print(f\"Performance moyenne sur {nb_episodes} épisodes: {moyenne_perf}\")\n",
    "    return moyenne_perf\n",
    "\n",
    "# Initialiser l'environnement et la graine\n",
    "env = gym.make('CartPole-v1')\n",
    "init_seed(5)\n",
    "\n",
    "# Évaluer et afficher les performances des meilleures politiques\n",
    "print(\"Évaluation de la meilleure politique déterministe:\")\n",
    "moyenne_perf_det = evaluer_politique(politique_det, env)\n",
    "\n",
    "print(\"\\nÉvaluation de la meilleure politique stochastique:\")\n",
    "moyenne_perf_stoc = evaluer_politique(politique_stoc, env)\n",
    "\n",
    "# Fermer l'environnement\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <span style=\"color:green\"> Analysez les résultats. Qu'en déduisez-vous sur le type de politique le plus adapté ici ?  </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>  \n",
    "La politique déterministe semble être le choix le plus adapté pour cet environnement, probablement en raison de sa capacité à fournir des actions plus cohérentes et prévisibles, ce qui est essentiel pour maintenir le poteau en équilibre. La politique stochastique, bien qu'elle puisse explorer plus de variations d'actions, n'a pas réussi à atteindre une performance compétitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tpdeeprl2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
